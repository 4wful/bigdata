# ğŸ“ˆ Proyecto: AnÃ¡lisis Financiero con Streaming y Power BI (Business Analytics)

Este repositorio contiene un sistema de anÃ¡lisis de datos bursÃ¡tiles con procesamiento en tiempo real y visualizaciÃ³n avanzada en **Power BI**. EstÃ¡ orientado al curso de Business Analytics, con foco en generar insights Ãºtiles para la toma de decisiones empresariales.

âœ”ï¸ Adaptado para funcionar en **WSL2 sin entorno grÃ¡fico (headless)**.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Apache Kafka** â†’ streaming de datos financieros  
- **Apache Spark** â†’ procesamiento ETL en tiempo real  
- **Python** â†’ transformaciÃ³n, modelado y predicciÃ³n  
- **Scikit-learn** â†’ regresiÃ³n lineal y random forest  
- **Power BI** â†’ visualizaciÃ³n de KPIs e insights  
- **Docker** â†’ orquestaciÃ³n de contenedores (Kafka + Zookeeper)

---

## ğŸ“ Estructura del proyecto

```
â”œâ”€â”€ config/ # ConfiguraciÃ³n central (paths, constantes)
â”œâ”€â”€ kafka_services/ # Docker Compose para Kafka y Zookeeper
â”œâ”€â”€ machine_learning/ # Entrenamiento y predicciÃ³n ML
â”œâ”€â”€ output/ # Archivos finales CSV, grÃ¡ficos, modelos
â”œâ”€â”€ producer/ # API â†’ Kafka (streaming)
â”œâ”€â”€ raw_data/ # CSV y PDF crudos
â”œâ”€â”€ spark/ # LÃ³gica de Spark Streaming
â”œâ”€â”€ static_etl/ # ETL estÃ¡tico (CSV/PDF)
â”œâ”€â”€ utils/ # Scripts auxiliares
â”œâ”€â”€ .env # Variables sensibles (no incluido en Git)
â”œâ”€â”€ .gitignore # Archivos y carpetas excluidos
â”œâ”€â”€ run_after_streaming.sh # Script automatizado del flujo completo
â”œâ”€â”€ requirements.txt # Dependencias del proyecto
â””â”€â”€ README.md # DocumentaciÃ³n general
```

## ğŸ§° Requisitos del sistema

- Ubuntu + WSL2 (opcional pero compatible y probado)
- Python 3.10 
- Java JDK 11 
- Apache Spark  
- Apache Hadoop 
- Docker Desktop  
- Power BI Desktop  

---

## ğŸš€ Pasos para ejecutar el proyecto

### 1. Clonar y preparar entorno

```bash
git clone https://github.com/4wful/business-analytics.git
cd business-analytics
python -m venv venv
source venv/bin/activate      # En Linux o WSL2
pip install -r requirements.txt
```

### 2. Configurar archivo

ğŸ” Este archivo no se sube al repositorio por seguridad.

Crea un archivo `.env` en la raÃ­z del proyecto con el siguiente contenido:

```env
API_KEY=tu_api_key_de_alpha_vantage
KAFKA_TOPIC=nombre_topic
KAFKA_BOOTSTRAP_SERVERS=localhost:9092,localhost:9093,localhost:9094
DEST_WIN_PATH=/mnt/c/Users/TU_USUARIO/Desktop/.../BusinessAnalytics/data
```

### 3. Levantar servicios Kafka

```bash
cd kafka_services
docker-compose up -d
```

Crear y verificar el tÃ³pico:

```bash
# Crear tÃ³pico
docker exec -it kafka-broker-1 kafka-topics --create \
  --bootstrap-server kafka-broker-1:29092 \
  --replication-factor 3 --partitions 3 \
  --topic nombre_topic --if-not-exists

# Listar tÃ³picos existentes
docker exec -it kafka-broker-1 kafka-topics --list \
  --bootstrap-server kafka-broker-1:29092
```

### 4. Ejecutar el flujo de datos

```bash
# AsegÃºrate de que Spark estÃ© instalado localmente
cd spark
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5 spark_api.py

# Ejecutar procesamiento completo post-streaming
./run_after_streaming.sh

Este script ejecuta:

EnvÃ­o de datos por producer/api_to_kafka.py

Espera y transformaciÃ³n vÃ­a Spark

ConversiÃ³n Parquet â†’ CSV

Limpieza estÃ¡tica de CSV y PDF

Entrenamiento de modelos ML (RegresiÃ³n Lineal y Random Forest)

Predicciones e inferencia

Copiado automÃ¡tico a Power BI

```
## ğŸ“Š VisualizaciÃ³n final en Power BI

DirÃ­gete a la carpeta output conecta Power BI a los archivos CSV necesarios para construir tu dashboard personalizado.

ğŸ“· Vista previa del Dashboard

![image](https://github.com/user-attachments/assets/1a1e4ade-ba48-49ef-80ea-af239573d592)

**KPIs sugeridos:**

- Precio real vs. predicho

- Volumen total negociado

- Tendencia por sector / regiÃ³n

- Errores de predicciÃ³n (RÂ², MSE)

## ğŸ“Œ Notas adicionales
Los archivos .pkl, .txt y .csv generados automÃ¡ticamente se excluyen con .gitignore.

El sistema estÃ¡ optimizado para funcionar sin entorno grÃ¡fico (modo headless en WSL2).

El flujo completo es reproducible y automatizado desde run_after_streaming.sh.

ğŸ‘¨â€ğŸ« AutorÃ­a
Grupo 4 â€“ Trabajo final para el curso Business Analytics (2025)
**Â¡Explora, ejecuta y aprende del flujo completo de datos en tiempo real!**

