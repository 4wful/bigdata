# ğŸ“ˆ Proyecto: AnÃ¡lisis de Mercados BursÃ¡tiles con Big Data y Power BI

Este repositorio presenta un sistema de **procesamiento y anÃ¡lisis de datos financieros en tiempo real**, utilizando tecnologÃ­as de **Big Data** como Apache Kafka y Spark, con visualizaciÃ³n avanzada en **Power BI**. El enfoque estÃ¡ orientado al curso de **Big Data**, aplicando sus principios al anÃ¡lisis del mercado bursÃ¡til para generar insights estratÃ©gicos.

âœ”ï¸ Optimizado para ejecutarse en **WSL2 en modo headless** (sin entorno grÃ¡fico).

---

## ğŸ› ï¸ TecnologÃ­as y herramientas clave

- **WSL2 + UBUNTU** â†’ entorno de trabajo necesario
- **Docker** â†’ contenedores para servicios como Kafka y Zookeeper
- **Apache Kafka** â†’ ingesta de datos en streaming desde APIs bursÃ¡tiles  
- **Apache Spark** â†’ procesamiento distribuido y ETL en tiempo real  
- **Python** â†’ lÃ³gica de negocio, transformaciÃ³n y predicciÃ³n  
- **Power BI** â†’ visualizaciÃ³n de indicadores clave del mercado  

---

## ğŸ“ Estructura del proyecto

```
â”œâ”€â”€ config/ # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ kafka_services/ # OrquestaciÃ³n Kafka + Zookeeper con Docker
â”œâ”€â”€ machine_learning/ # Entrenamiento y predicciÃ³n de modelos ML
â”œâ”€â”€ output/ # Resultados (CSV, grÃ¡ficos, modelos)
â”œâ”€â”€ producer/ # Ingesta de datos bursÃ¡tiles â†’ Kafka
â”œâ”€â”€ raw_data/ # Archivos CSV y PDF originales
â”œâ”€â”€ spark/ # LÃ³gica de procesamiento con Spark Streaming
â”œâ”€â”€ static_etl/ # ETL tradicional sobre archivos planos
â”œâ”€â”€ utils/ # Funciones auxiliares
â”œâ”€â”€ .env # Variables sensibles (ignorado por Git)
â”œâ”€â”€ .gitignore # Exclusiones de control de versiones
â”œâ”€â”€ run_after_streaming.sh# Pipeline automÃ¡tico post-streaming
â”œâ”€â”€ requirements.txt # Dependencias Python
â””â”€â”€ README.md # DocumentaciÃ³n del proyecto
```

---

## âš™ï¸ Requisitos del sistema

- Ubuntu con WSL2 
- Python 3.10 
- Java JDK 11 
- Apache Spark  
- Apache Hadoop 
- Docker Desktop  
- Power BI Desktop  

---

## ğŸš€ EjecuciÃ³n del proyecto

### 1. ClonaciÃ³n e instalaciÃ³n de entorno

```bash
git clone https://github.com/4wful/bigdata.git
cd bigdata
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configurar archivo

ğŸ” Este archivo no se sube al repositorio por seguridad.

Crea un archivo `.env` en la raÃ­z del proyecto con el siguiente contenido:

```env
API_KEY=tu_api_key_de_alpha_vantage
KAFKA_TOPIC=nombre_topic
KAFKA_BOOTSTRAP_SERVERS=localhost:9092,localhost:9093,localhost:9094
DEST_WIN_PATH=/mnt/c/Users/TU_USUARIO/Desktop/.../bigdata/data
```

### 3. Levantar servicios Kafka

```bash
cd kafka_services
docker-compose up -d
```

Crear y verificar el tÃ³pico:

```bash
# Crear tÃ³pico
docker exec -it kafka-broker-1 kafka-topics --create \
  --bootstrap-server kafka-broker-1:29092 \
  --replication-factor 3 --partitions 3 \
  --topic nombre_topic --if-not-exists

# Listar tÃ³picos existentes
docker exec -it kafka-broker-1 kafka-topics --list \
  --bootstrap-server kafka-broker-1:29092
```

### 4. Ejecutar el flujo de datos

```bash
# AsegÃºrate de que Spark estÃ© instalado localmente
cd spark
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5 spark_api.py

# Ejecutar procesamiento completo post-streaming
./run_after_streaming.sh

Este script ejecuta:

EnvÃ­o de datos por producer/api_to_kafka.py

Espera y transformaciÃ³n vÃ­a Spark

ConversiÃ³n Parquet â†’ CSV

Limpieza estÃ¡tica de CSV y PDF

Entrenamiento de modelos ML (RegresiÃ³n Lineal y Random Forest)

Predicciones e inferencia

Copiado automÃ¡tico a Power BI

```
## ğŸ“Š VisualizaciÃ³n final en Power BI

DirÃ­gete a la carpeta output conecta Power BI a los archivos CSV necesarios para construir tu dashboard personalizado.

ğŸ“· Vista previa del Dashboard

![image](https://github.com/user-attachments/assets/447c18f4-74cf-4cd2-b18a-b46ccbbf313b)

## ğŸ“Œ Notas adicionales
Los archivos .pkl, .txt y .csv generados automÃ¡ticamente se excluyen con .gitignore.

El sistema estÃ¡ optimizado para funcionar sin entorno grÃ¡fico (modo headless en WSL2).

El flujo completo es reproducible y automatizado desde run_after_streaming.sh.

ğŸ‘¨â€ğŸ« AutorÃ­a
Grupo 3 â€“ Trabajo final para el curso de Big Data (2025)
**Â¡Explora, ejecuta y aprende del flujo completo de datos en tiempo real!**

