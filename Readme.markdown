# ğŸ“ˆ Proyecto: Real-Time Stock Forecasting Pipeline (Big Data)

Este repositorio contiene un sistema completo de procesamiento de datos bursÃ¡tiles en tiempo real utilizando tecnologÃ­as Big Data como **Apache Kafka**, **Apache Spark** y **Machine Learning**. Desarrollado como trabajo final del curso **Big Data**.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Apache Kafka**: streaming de datos financieros  
- **Apache Spark**: procesamiento ETL en tiempo real  
- **Python**: transformaciÃ³n, modelado y predicciÃ³n  
- **Power BI**: visualizaciÃ³n de KPIs, mÃ©tricas e insights  
- **Docker**: orquestaciÃ³n de contenedores (Kafka)

## ğŸ“ Estructura general del proyecto

```
â”œâ”€â”€ config/
â”œâ”€â”€ kafka_services/
â”œâ”€â”€ machine_learning/
â”œâ”€â”€ output/                # Datos procesados para usar en Power BI
â”œâ”€â”€ power_bi/              # Reporte Power BI (.pbix)
â”œâ”€â”€ producer/
â”œâ”€â”€ raw_data/              # Datos crudos CSV y PDF
â”œâ”€â”€ spark/
â”œâ”€â”€ static_etl/
â”œâ”€â”€ utils/
â”œâ”€â”€ .env                   # Variables de entorno sensibles
â””â”€â”€ requirements.txt
```

## ğŸ§° Requisitos del sistema

Instala los siguientes componentes antes de ejecutar el proyecto:

- Python 3.8+  
- Java JDK 8 âœ… obligatorio para Spark  
- Apache Hadoop (cliente local)  
- Apache Spark (local)  
- Docker Desktop (para correr Kafka/Zookeeper)  
- Power BI Desktop (para abrir los reportes)

## ğŸš€ Pasos para ejecutar el proyecto

### 1. Clonar y preparar entorno

```bash
git clone <URL_REPOSITORIO>
cd nombre-del-proyecto
python -m venv venv
source .\venv\Scripts\Activate.ps1 
pip install -r requirements.txt
```

### 2. Configurar variables sensibles

Crea un archivo `.env` en la raÃ­z del proyecto con el siguiente contenido:

```env
API_KEY=tu_api_key_aqui  # (ALPHA VANTAGE API)
KAFKA_TOPIC=nombre_de_tu_topic
KAFKA_BOOTSTRAP_SERVERS=tus_local_host
```

### 3. Levantar servicios Kafka

```bash
cd kafka_services
docker-compose up --build
```

Crear y verificar el tÃ³pico:

```bash
# Crear tÃ³pico
docker exec -it kafka-broker-1 kafka-topics --create \
  --bootstrap-server kafka-broker-1:29092 \
  --replication-factor 3 --partitions 3 \
  --topic mercados-bursatiles --if-not-exists

# Verificar tÃ³picos
docker exec -it kafka-broker-1 kafka-topics --list \
  --bootstrap-server kafka-broker-1:29092
```

### 4. Ejecutar el flujo de datos

```bash
# a. Enviar datos a Kafka
python producer/api_to_kafka.py

# b. Procesar datos en Spark Streaming
.\run_spark_api.ps1

# c. ConversiÃ³n Parquet a CSV
.\run_parquet_csv.ps1

# d. ETL estÃ¡tico desde PDF
.\run_static_etl.ps1
```

### 5. Entrenar y usar el modelo de ML

```bash
python machine_learning/model_train.py
python machine_learning/inference.py
```

## ğŸ“Š VisualizaciÃ³n en Power BI

Los archivos procesados estÃ¡n en `output/dashboard_data/` y pueden ser conectados directamente a Power BI.

ğŸ“Œ *PrÃ³ximamente se aÃ±adirÃ¡ una imagen de referencia del dashboard final.*


## ğŸ‘¨â€ğŸ« Autor

**Grupo 3** â€“ Trabajo final para el curso **Big Data** (AÃ±o 2025), dictado por el docente **Lira Camacho**.

Â¡Explora, ejecuta y aprende del flujo completo de datos en tiempo real! ğŸ“ˆ

> **Todos los derechos reservados al Grupo 3.**